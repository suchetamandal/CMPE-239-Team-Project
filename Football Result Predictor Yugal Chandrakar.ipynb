{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from random import sample\n",
    "from time import time\n",
    "import sklearn.ensemble as sk\n",
    "import sklearn.metrics as skm\n",
    "import pylab as pl\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "database ='database.sqlite'\n",
    "connection = sqlite3.connect(database)\n",
    "match = pd.read_sql(\"SELECT * FROM Match;\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping unwanted NaN rows from the dataset\n",
    "selectColumns=['home_team_api_id', 'away_team_api_id', 'home_team_goal', 'away_team_goal', \n",
    "                'date',  'country_id', 'league_id', 'season', 'stage']\n",
    "match.dropna(subset = selectColumns, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Selection\n",
    "selectColumns=[\n",
    "               'home_team_api_id','away_team_api_id','home_team_goal','away_team_goal','date',\n",
    "               'country_id','league_id','season','stage','B365H','BWH','IWH','LBH','PSH',\n",
    "               'B365D','BWD','IWD','LBD','PSD','B365A','BWA','IWA','LBA', 'PSA','home_player_7',\n",
    "               'away_player_7','home_player_10','home_player_11','away_player_10','away_player_11'\n",
    "              ]\n",
    "\n",
    "#Selecting important columns only\n",
    "mBet=match[selectColumns]\n",
    "\n",
    "#Converting data values to required format\n",
    "mBet['date'] = pd.to_datetime(mBet['date'])\n",
    "mBet = mBet.assign(month=mBet['date'].dt.month)\n",
    "mBet['date'] = mBet['date'].dt.year\n",
    "mBet['season']= mBet['season'].str[:4]\n",
    "\n",
    "# Replacing null values with mean values\n",
    "mBet['BWH'].fillna(mBet['BWH'].mean(),inplace=True)\n",
    "mBet['BWA'].fillna(mBet['BWA'].mean(),inplace=True)\n",
    "mBet['BWD'].fillna(mBet['BWD'].mean(),inplace=True)\n",
    "mBet['IWH'].fillna(mBet['BWH'].mean(),inplace=True)\n",
    "mBet['IWA'].fillna(mBet['BWA'].mean(),inplace=True)\n",
    "mBet['IWD'].fillna(mBet['BWD'].mean(),inplace=True)\n",
    "mBet['LBH'].fillna(mBet['BWH'].mean(),inplace=True)\n",
    "mBet['LBA'].fillna(mBet['BWA'].mean(),inplace=True)\n",
    "mBet['LBD'].fillna(mBet['BWD'].mean(),inplace=True)\n",
    "mBet['PSH'].fillna(mBet['BWH'].mean(),inplace=True)\n",
    "mBet['PSA'].fillna(mBet['BWA'].mean(),inplace=True)\n",
    "mBet['PSD'].fillna(mBet['BWD'].mean(),inplace=True)\n",
    "mBet['B365H'].fillna(mBet['B365H'].mean(),inplace=True)\n",
    "mBet['B365A'].fillna(mBet['B365A'].mean(),inplace=True)\n",
    "mBet['B365D'].fillna(mBet['B365D'].mean(),inplace=True)\n",
    "mBet.fillna(0,inplace=True)\n",
    "\n",
    "matchData=mBet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train data 18185\n",
      "Length of Test data 3898\n",
      "Length of Validate data 3896\n"
     ]
    }
   ],
   "source": [
    "# Creating train, test and validate data from a single data\n",
    "def train_test_validate_split(dataframe, trainPercent=.7, validatePercent=.15, seed=None):\n",
    "    # Referred from StackOverflow\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(dataframe.index)\n",
    "    length = len(dataframe)\n",
    "    trainEnd = int(trainPercent * length)\n",
    "    validateEnd = int(validatePercent * length) + trainEnd\n",
    "    train = dataframe.ix[perm[:trainEnd]]\n",
    "    test = dataframe.ix[perm[validateEnd:]]\n",
    "    validate = dataframe.ix[perm[trainEnd:validateEnd]]\n",
    "    return train, test, validate\n",
    "\n",
    "np.random.seed([243])\n",
    "train, test, validate = train_test_validate_split(matchData)\n",
    "\n",
    "# Length of all different data\n",
    "print(\"Length of Train data %d\" %len(train))\n",
    "print(\"Length of Test data %d\" %len(test))\n",
    "print(\"Length of Validate data %d\" %len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fullTime(h,a):\n",
    "    if (h>a) : \n",
    "        return \"Win\"\n",
    "    elif (h<a) : \n",
    "        return \"Loss\"\n",
    "    else:\n",
    "        return \"Draw\"\n",
    "# Fetching full time results of the different data sets i.e. Test, Train and Validate\n",
    "testResult=test.apply(lambda row: fullTime(row['home_team_goal'], row['away_team_goal']), axis=1)\n",
    "trainResult=train.apply(lambda row: fullTime(row['home_team_goal'], row['away_team_goal']), axis=1)\n",
    "validateResult=validate.apply(lambda row: fullTime(row['home_team_goal'], row['away_team_goal']), axis=1)\n",
    "\n",
    "# Deleting goals column from all data, so that we can predict and get desired results\n",
    "# If we don't remove them, it'll give actual results which will fail our purpose\n",
    "\n",
    "del train['home_team_goal']\n",
    "del train['away_team_goal']\n",
    "del test['home_team_goal']\n",
    "del test['away_team_goal']\n",
    "del validate['home_team_goal']\n",
    "del validate['away_team_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Win     8405\n",
       "Loss    5188\n",
       "Draw    4592\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainResult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applying the different classifier's and predicting the results\n",
    "\n",
    "clfKNN = neighbors.KNeighborsClassifier(30, weights = 'uniform') \n",
    "# Results were validation set = 0.463715 & test set = 0.459407 # Dropper it\n",
    "\n",
    "# clf=LogisticRegression(random_state=0)\n",
    "# Results were validation set = 0.520115 & test set = 0.514813\n",
    "\n",
    "clfLR=LogisticRegression(penalty='l1', C=10)\n",
    "# Results were validation set = 0.522618 & test set = 0.520970 # Not getting desired results, Dropped it\n",
    "\n",
    "clfDT= DecisionTreeClassifier(random_state=100)\n",
    "# Results were validation set = 0.397690 & test set = 0.402462 # Worst Result\n",
    "\n",
    "clfSVC=SVC(kernel='rbf', random_state=0, gamma=.01, C=1)\n",
    "# Results were validation set = 0.461983 & test set = 0.455175 # Not getting desired results, Dropped it\n",
    "\n",
    "clfMNB=MultinomialNB()\n",
    "\n",
    "# Going forward with Random Forest Classfier and more tuning\n",
    "\n",
    "# When adding different parameters or levers, accuracy kept increasing\n",
    "# With n_estimators only accuracy was 0.501542 & 0.493981\n",
    "# When adding oob_score nothing much changed  accuracy was 0.502 & 0.495 but processing time was somewhat faster than before\n",
    "# When adding random_state accuracy was 0.510491 & 0.495575\n",
    "# When adding max_features accuracy didn't changed 0.510491 & 0.495575\n",
    "# When added min_samples_leaf, accuracy made a good gap (atleast for me) from before 0.520693 & 0.514044\n",
    "# By increasing n_estimators value from 100->1000 (10 Times), processing time increased and better accuracy 0.524350 & 0.530589\n",
    "# By increasing n_estimators value from 1000->5000 (5 Times), not much difference in accuracy 0.524158 & 0.531743\n",
    "# but processing time is unacceptably long.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clfRF = sk.RandomForestClassifier(n_estimators=1000, oob_score = True,random_state =42,max_features = \"auto\", min_samples_leaf = 50)\n",
    "model = clfRF.fit(train, trainResult)\n",
    "model = clfKNN.fit(train, trainResult)\n",
    "model = clfLR.fit(train, trainResult)\n",
    "model = clfDT.fit(train, trainResult)\n",
    "model = clfSVC.fit(train, trainResult)\n",
    "model = clfMNB.fit(train, trainResult)\n",
    "\n",
    "RF=clfRF.score(test, testResult)\n",
    "KNN=clfKNN.score(test, testResult)\n",
    "LR=clfLR.score(test, testResult)\n",
    "DT=clfDT.score(test, testResult)\n",
    "SVC=clfSVC.score(test, testResult)\n",
    "MNB=clfMNB.score(test, testResult)\n",
    "\n",
    "perf=[RF,KNN,LR,DT,SVC,MNB] # Values of different classifiers \n",
    "\n",
    "objects = ('RF','KNN','LR','DT','SVC','MNB') # Names of different classifierss value\n",
    "y_pos = np.arange(len(objects))\n",
    "plt.ylim(0,0.60) # Increased y-axis limit for better visualization\n",
    "plt.bar(y_pos, perf, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy Scores')\n",
    "plt.title('Different Classifier Used')\n",
    "\n",
    "#print(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfRF = sk.RandomForestClassifier(n_estimators=1000, oob_score = True,random_state =42,max_features = \"auto\", min_samples_leaf = 50)\n",
    "model = clfRF.fit(train, trainResult)\n",
    "\n",
    "validatePredictions = clfRF.predict(validate)\n",
    "print(\"Calculated mean accuracy score of the Model for validation set = %f\" %(clfRF.score(validate, validateResult)))\n",
    "\n",
    "testPredictions = clfRF.predict(test)\n",
    "print(\"Calculated mean accuracy score of the Model for test set = %f\" %(clfRF.score(test, testResult)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theResultofValidate = skm.confusion_matrix(validateResult,validatePredictions)\n",
    "f1=skm.f1_score()\n",
    "pl.matshow(theResultofValidate)\n",
    "pl.title('Confusion matrix for Validate data\\n\\n')\n",
    "pl.colorbar()\n",
    "pl.show()\n",
    "print('Prediction of Validate data')\n",
    "pd.crosstab(validateResult, validatePredictions, rownames=['Actual Results'], colnames=['Predicted Results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theResultofTest = skm.confusion_matrix(testResult,testPredictions)\n",
    "pl.matshow(theResultofTest)\n",
    "pl.title('Confusion matrix for Test data\\n\\n')\n",
    "pl.colorbar()\n",
    "pl.show()\n",
    "print('Prediction of Test data')\n",
    "pd.crosstab(testResult, testPredictions, rownames=['Actual Results'], colnames=['Predicted Results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "objects = ('Python', 'C++', 'Java', 'Perl', 'Scala', 'Lisp')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [10,8,6,4,2,1]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=.2)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    "plt.ylim(0,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
